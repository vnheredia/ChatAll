import streamlit as st
import os
import hashlib
import chromadb
import google.generativeai as genai

from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from processor import process_uploaded_file
from utils import save_temp_file


# ============================================================
# CONFIGURACI√ìN GENERAL
# ============================================================
st.set_page_config(page_title="Chat PDF con Gemini")

# Carga variables de entorno desde .env
# Aqu√≠ se espera GOOGLE_API_KEY=xxxx
load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Modelo de embeddings local
# Se puede cambiar por otros modelos de sentence-transformers
EMBEDDING_MODEL = SentenceTransformer("all-MiniLM-L6-v2")

# Se Inicializa el Cliente de ChromaDB
client = chromadb.Client()

# ============================================================
# SESSION STATE
# ============================================================
# session_state nos permite "recordar" cosas entre reruns.
if "collection" not in st.session_state:
    st.session_state.collection = None

if "pdf_processed" not in st.session_state:
    st.session_state.pdf_processed = False

if "pdf_hash" not in st.session_state:
    st.session_state.pdf_hash = None

# ============================================================
# FUNCIONES
# ============================================================
def hash_pdf(file) -> str:
    return hashlib.sha256(file.getvalue()).hexdigest()

def extract_text_from_file(uploaded_file):
    return process_uploaded_file(uploaded_file)
 


def chunk_text(text):
    """
    Divide un texto largo en fragmentos (chunks) con solapamiento.

    chunk_size:
        - N√∫mero m√°ximo de caracteres por fragmento
        - Valores t√≠picos: 400‚Äì800
        - M√°s grande = m√°s contexto, pero embeddings m√°s caros

    overlap:
        - N√∫mero de caracteres que se repiten entre chunks consecutivos
        - Evita que una idea quede cortada entre fragmentos
        - Regla com√∫n: 10‚Äì20% del chunk_size

    Devuelve:
        Lista de diccionarios, cada uno representando un chunk con:
        - id           -> identificador √∫nico
        - content      -> texto del fragmento
        - start_index  -> posici√≥n donde comienza en el texto original
        - size         -> longitud real del chunk
    """
    chunk_size = 500 
    overlap = 100
    chunks = []          # Aqu√≠ guardaremos todos los fragmentos
    start = 0            # Puntero que indica desde d√≥nde empezamos a cortar
    chunk_id = 0         # Contador para asignar IDs √∫nicos

    # El while se ejecuta mientras NO hayamos llegado al final del texto
    while start < len(text):

        # 1Ô∏è‚É£ Cortamos el texto desde 'start' hasta 'start + chunk_size'
        #    Python corta autom√°ticamente si se pasa del largo del texto
        chunk_text = text[start:start + chunk_size]

        # 2Ô∏è‚É£ Guardamos el chunk junto con metadata √∫til
        chunks.append({
            "id": f"chunk_{chunk_id}",   # Identificador √∫nico del fragmento
            "content": chunk_text,       # Texto real del fragmento
            "start_index": start,        # Posici√≥n en el texto original
            "size": len(chunk_text)      # Tama√±o real del fragmento
        })

        # 3Ô∏è‚É£ Incrementamos el ID para el pr√≥ximo chunk
        chunk_id += 1

        # 4Ô∏è‚É£ Avanzamos el puntero 'start'
        #    No avanzamos chunk_size completo,
        #    sino (chunk_size - overlap) para que haya solapamiento
        #
        #    Ejemplo:
        #    chunk_size = 500
        #    overlap    = 100
        #    start avanza 400 caracteres
        #
        #    Los √∫ltimos 100 caracteres del chunk actual
        #    aparecer√°n tambi√©n al inicio del siguiente
        start += chunk_size - overlap

    # 5Ô∏è‚É£ Cuando start >= len(text), el while termina
    #    y devolvemos todos los fragmentos creados
    return chunks



def create_chroma_collection(chunks):
    """
    Crea una colecci√≥n nueva en ChromaDB a partir de los chunks generados.

    Cada chunk se almacena junto con:
    - su embedding (vector num√©rico)
    - su texto original
    - metadata √∫til
    """

    # ------------------------------
    # 1Ô∏è‚É£ Borrado defensivo
    # ------------------------------
    # Si ya existe una colecci√≥n con el mismo nombre ("pdf_rag"),
    try:
        client.delete_collection("pdf_rag")
    except:
        # Si la colecci√≥n no existe, Chroma lanza error.
        # Lo ignoramos porque es un caso esperado.
        pass

    # ------------------------------
    # 2Ô∏è‚É£ Crear colecci√≥n nueva
    # ------------------------------
    # Aqu√≠ Chroma crea:
    # - una tabla de documentos
    # - un √≠ndice vectorial
    # - espacio para metadatos
    collection = client.create_collection(name="pdf_rag")

    # ------------------------------
    # 3Ô∏è‚É£ Separar texto de metadata
    # ------------------------------
    # Extraemos SOLO el contenido textual de cada chunk.
    # Esto es lo que se convertir√° en embeddings.
    texts = [c["content"] for c in chunks]

    # ------------------------------
    # 4Ô∏è‚É£ Generar embeddings
    # ------------------------------
    # El modelo de SentenceTransformers convierte cada texto
    # en un vector num√©rico.
    #
    # Cada vector representa el significado del chunk.
    embeddings = EMBEDDING_MODEL.encode(texts)

    # ------------------------------
    # 5Ô∏è‚É£ Insertar datos en Chroma
    # ------------------------------
    collection.add(
        # Texto original del chunk
        documents=texts,

        # Vectores que permiten b√∫squeda sem√°ntica
        embeddings=embeddings.tolist(),

        # IDs √∫nicos
        # Sirven para identificar cada chunk internamente
        ids=[c["id"] for c in chunks],

        # Metadata asociada a cada chunk
        metadatas=[
            {
                "chunk_index": i,         # Orden del chunk
                "start_index": c["start_index"],  # Posici√≥n en el texto original
                "chunk_size": c["size"]   # Tama√±o real del fragmento
            }
            for i, c in enumerate(chunks)
        ]
    )

    # ------------------------------
    # 6Ô∏è‚É£ Devolver colecci√≥n lista
    # ------------------------------
    # La colecci√≥n ya puede:
    # - recibir queries (preguntas)
    # - devolver chunks relevantes
    return collection



def retrieve_context(collection, query, k=4):
    """
    Recupera los k chunks m√°s similares a la pregunta.
    Devuelve tanto el texto como la metadata asociada.
    """
    query_embedding = EMBEDDING_MODEL.encode([query])

    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=k
    )

    return results


def ask_gemini(context, question):
    """
    Llama a Gemini usando el contexto recuperado.
    El prompt fuerza comportamiento RAG (no inventar).
    """
    model = genai.GenerativeModel("models/gemini-2.5-flash-lite")

    prompt = f"""
Eres un asistente que responde SOLO con la informaci√≥n del contexto.
Si la respuesta no est√° en el contexto, di: "No se encuentra en el documento".

Contexto:
{context}

Pregunta:
{question}
"""

    response = model.generate_content(prompt)
    return response.text

# ============================================================
# INTERFAZ
# ============================================================

st.title("üìÑ Chat con PDF + ChromaDB + Gemini")

uploaded_pdf = st.file_uploader(
    "Sube un archivo",
    type=["pdf", "txt", "docx", "pptx", "xlsx"]
)


# üîÑ Detectar cambio de PDF y resetear estado
if uploaded_pdf:
    current_hash = hash_pdf(uploaded_pdf)

    if st.session_state.pdf_hash != current_hash:
        st.session_state.pdf_hash = current_hash
        st.session_state.pdf_processed = False
        st.session_state.collection = None

# ------------------------------
# BOT√ìN PROCESAR PDF
# ------------------------------
if uploaded_pdf and not st.session_state.pdf_processed:
    if st.button("üì• Procesar Archivo"):
        with st.spinner("Procesando Archivo..."):
            text = extract_text_from_file(uploaded_pdf)
            chunks = chunk_text(text)
            st.session_state.collection = create_chroma_collection(chunks)
            st.session_state.pdf_processed = True

        st.success(f"Archivos procesado ‚úÖ ({len(chunks)} fragmentos)")

# ------------------------------
# SECCI√ìN DE PREGUNTAS
# ------------------------------
if st.session_state.pdf_processed and st.session_state.collection:
    st.divider()
    st.subheader("‚ùì Pregunta al documento")

    question = st.text_input("Escribe tu pregunta")

    if st.button("ü§ñ Preguntar") and question:
        with st.spinner("Buscando respuesta..."):
            results = retrieve_context(st.session_state.collection, question)

            # Unimos los documentos para Gemini
            context_text = "\n\n".join(results["documents"][0])

            answer = ask_gemini(context_text, question)

        st.subheader("ü§ñ Respuesta")
        st.write(answer)

        # ------------------------------
        # DETALLE DEL CONTEXTO USADO
        # ------------------------------
        with st.expander("üìö Contexto usado (detallado)"):
            for i, (doc, meta) in enumerate(
                zip(results["documents"][0], results["metadatas"][0])
            ):
                st.markdown(f"""
**Chunk #{meta['chunk_index']}**
- üìç Inicio en texto: `{meta['start_index']}`
- üìè Tama√±o: `{meta['chunk_size']}` caracteres

```text
{doc}
""")